{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded, proceed to training\n",
      "Starting Training for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\codes\\tAIko\\dataloader\\taiko_datasets.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(processed_audio), difficulty, torch.tensor(array)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n",
      "audio torch.Size([4, 1, 128, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_note_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_note_prediction\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m NotePredictionModel(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m17\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_note_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/music_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\codes\\tAIko\\train_note_prediction.py:84\u001b[0m, in \u001b[0;36mtrain_note_prediction\u001b[1;34m(model, music_folder, ideal_size, epochs, continue_checkpoint, save_checkpoint_dir)\u001b[0m\n\u001b[0;32m     81\u001b[0m number_of_runs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Print statistics\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m combined_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_of_runs \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1999\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models.note_prediction import NotePredictionModel\n",
    "from torchsummary import summary\n",
    "from train_note_prediction import train_note_prediction\n",
    "\n",
    "model = NotePredictionModel(128, 64, 17)\n",
    "\n",
    "train_note_prediction(model,\"./data/music_test\",(128,128),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded, proceed to training\n",
      "Starting Training for epoch 0\n",
      "Finished Training for epoch 0\n",
      "epoch=0 loss: 16.784024700654292\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kana_\\.conda\\envs\\taiko-ai\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "d:\\codes\\tAIko\\dataloader\\audio_pipeline.py:44: UserWarning: The input to TimeStretch must be complex type. Providing non-complex tensor produces invalid results.\n",
      "  stretched = self.stretch(spec_resampled, factor).real\n"
     ]
    }
   ],
   "source": [
    "# Load audio\n",
    "\n",
    "from dataloader.audio_pipeline import AudioTransformPipeline\n",
    "from utils.osu_reader.osu_reader import OsuTaikoReader\n",
    "\n",
    "\n",
    "reader = OsuTaikoReader(\"./data/music_test/chikyuu/Mitani Nana - Chikyuu Saigo no Kokuhaku o (Star Stream) [Lundle's Oni].osu\")\n",
    "reader.audio.load_waveform()\n",
    "reader.timing_points.beats_point(10000)\n",
    "\n",
    "audio = reader.audio[5280:6995]\n",
    "\n",
    "transformer = AudioTransformPipeline(reader.audio.sample_rate(), 128, 128)\n",
    "processed_audio = transformer(audio)\n",
    "processed_audio = processed_audio.unsqueeze(0)\n",
    "\n",
    "\n",
    "print(processed_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5663, -1.1595, -0.3509, -0.2307,  0.5846, -0.7457,  0.1773, -0.2199,\n",
       "          0.7900, -0.9361,  0.4285, -0.5012,  0.5931, -0.0542,  0.3987,  0.0487,\n",
       "          0.1097]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import cast\n",
    "\n",
    "from utils.type import NotePredictionCheckpoint\n",
    "\n",
    "a = torch.load(\"note_prediction_model_0.pt\")\n",
    "a = cast(NotePredictionCheckpoint, a)\n",
    "we = a.modeldict\n",
    "\n",
    "model = NotePredictionModel(128, 64, 17)\n",
    "model.load_state_dict(we)\n",
    "\n",
    "model.eval()\n",
    "diff = torch.tensor([2])\n",
    "diff = diff.unsqueeze(0)\n",
    "# diff.unsqueeze(-1)\n",
    "print(diff.shape)\n",
    "\n",
    "model.predict(processed_audio,diff)\n",
    "# we"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taiko-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
